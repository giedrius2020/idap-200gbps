{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2dc18e-0ce9-4a97-8615-4aaf87a5ab22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a1114-6400-4a65-adfc-229b7bcdb112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "import dask\n",
    "import hist.dask\n",
    "import awkward as ak\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "from functools import partial\n",
    "import cloudpickle\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "\n",
    "import ndcctools.taskvine as vine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbebe2a-491e-4ad8-a9b2-7de656ab3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work for coffea\n",
    "def do_stuff(events):\n",
    "    import awkward as ak\n",
    "    import time\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # track number of events\n",
    "    num_entries = ak.num(events, axis=0)\n",
    "    _counter = 0\n",
    "\n",
    "    # read out all other branches into integers to avoid memory issues\n",
    "    for b in [\n",
    "        events.GenPart.pt,\n",
    "        events.GenPart.eta,\n",
    "        events.GenPart.phi,\n",
    "        # events.CorrT1METJet.phi,\n",
    "        # events.GenJet.pt,\n",
    "        # events.CorrT1METJet.eta,\n",
    "        # events.SoftActivityJet.pt,\n",
    "        # events.Jet.eta,\n",
    "        # events.Jet.phi,\n",
    "    ]: \n",
    "        _counter += ak.count_nonzero(b, axis=None)\n",
    "  \n",
    "    \n",
    "    return {\"chunks\": 1, \"num_entries\": num_entries, \"_counter\": _counter, \"runtime\": time.time() - t0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a2baf-28cd-48dc-a889-3e3cbd730a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the main taskvine scheduler specialized on executing dask graphs\n",
    "vine_scheduler = vine.DaskVine(port=8786, ssl=True,\n",
    "                            name=f\"{os.environ.get('USER', 'noname')}-coffea-casa\",\n",
    "                            run_info_path=\"/mnt/data/btovar-logs/\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865052d-ec00-4fba-8eb9-91eda5af7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the token files and their environment variables\n",
    "extra_files = {}\n",
    "env_vars = {}\n",
    "\n",
    "token_acc_path = \"/etc/cmsaf-secrets-chown/access_token\"\n",
    "token_xch_path = \"/etc/cmsaf-secrets-chown/xcache_token\"\n",
    "\n",
    "if Path(token_acc_path).is_file():\n",
    "    extra_files[vine_scheduler.declare_file(token_acc_path, cache=True)] = \"access_token\"\n",
    "    env_vars[\"BEARER_TOKEN_FILE\"] = \"access_token\"\n",
    "\n",
    "if Path(token_xch_path).is_file():\n",
    "    extra_files[vine_scheduler.declare_file(token_xch_path, cache=True)] = \"xcache_token\"\n",
    "    env_vars[\"XCACHE_FILE\"] = \"xcache_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df335683-f8b4-479a-8fa5-3d7a266eb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually we put all these options directly into dask calls,\n",
    "# but coffea preprocessing only allows one argument to set the scheduler,\n",
    "# thus we create a partial of manager.get, which is the function that takes\n",
    "# a dask graph and executes it.\n",
    "vine_get = partial(vine_scheduler.get,\n",
    "                    resources={\"cores\": 1},  #  max 1 core, 5GB of disk per task\n",
    "                    resources_mode='fixed',   # set to \"fixed\" to kill tasks on resources\n",
    "                    extra_files=extra_files,\n",
    "                    env_vars=env_vars,\n",
    "                    worker_transfers=True,  # keep partials at workers\n",
    "                    task_mode=\"function-calls\", # use one interpreter per worker\n",
    "                    lib_resources={\"cores\": 8, \"slots\": 8}, # resources a single interpreter can run\n",
    "                    # environment=\"env.tar.gz\", # nfor task_mode=\"tasks\" if taskvine version at worker \n",
    "                                                # is behind:\n",
    "                                                # poncho_package_create $CONDA_PREFIX env.tar.gz,\n",
    "                                                # or if more modules are needed at the execution site.\n",
    "                        )\n",
    "\n",
    "# given to coffea and dask functions as **scheduler_options to make taskvine the scheduler\n",
    "scheduler_options = {}\n",
    "scheduler_options['scheduler'] = vine_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2217faf-b4db-47bb-8950-c1fdf224f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "import json\n",
    "fname = \"zstd_files.json\"\n",
    "\n",
    "files_to_add = 10\n",
    "\n",
    "fileset = {}\n",
    "with open(fname,'r') as fp:\n",
    "    for i,(dataset_name,file_list) in enumerate(json.load(fp).items()):\n",
    "        fileset[dataset_name] = {\"files\": {}}\n",
    "        for j,dataset_fpath in enumerate(file_list):\n",
    "            xrd_fpath = f\"root://xcache.cmsaf-dev.flatiron.hollandhpc.org:1094/{dataset_fpath}\"\n",
    "            fileset[dataset_name][\"files\"][xrd_fpath] = \"Events\"\n",
    "            files_to_add -= 1\n",
    "            if files_to_add < 1:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecaa10-f3c0-4f82-9d39-837e49d95c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "# step_size = 50_000\n",
    "# step_size = 100_000\n",
    "step_size = 250_000\n",
    "# step_size = 500_000\n",
    "# step_size = 5_000_000\n",
    "pre_filename = f\"preprocessed_{step_size}_demo_day.pkl\"\n",
    "\n",
    "try:\n",
    "    # do not re preprocess if we don't have too...\n",
    "    with open(pre_filename + \"never\", \"rb\") as f:\n",
    "        samples = cloudpickle.load(f)\n",
    "except Exception:\n",
    "    samples, report = dataset_tools.preprocess(fileset,\n",
    "                                               step_size=step_size,\n",
    "                                               skip_bad_files=True,\n",
    "                                               uproot_options={\n",
    "                                                   \"allow_read_errors_with_report\": True},\n",
    "                                               **scheduler_options)\n",
    "    with open(pre_filename, \"wb\") as f:\n",
    "        cloudpickle.dump(samples, f)\n",
    "\n",
    "total_files  = sum([len(p[\"files\"]) for p in samples.values()])\n",
    "total_chunks = sum(sum(len(f[\"steps\"]) for f in p[\"files\"].values()) for p in samples.values())\n",
    "print(f\"nfiles: {total_files} chunks: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3752d-9a08-4bdd-9d25-9771eb314153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular coffea\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# change default scheduler\n",
    "tasks = dataset_tools.apply_to_fileset(do_stuff,\n",
    "                                       samples,\n",
    "                                       uproot_options = {\n",
    "                                           \"allow_read_errors_with_report\":(OSError, TypeError, KeyError)})\n",
    "\n",
    "#(out, report) = dask.compute(tasks, **scheduler_options, progress_label=\"[green]process\")\n",
    "((out,report),) = dask.compute(tasks, **scheduler_options)\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"wall time: {t1 - t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac1e46-ef6f-4931-95a2-d05fb6478811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total time spent in uproot reading data: {ak.sum([v['duration'] for v in report.values()]):.2f} s\")\n",
    "print(f\"wall time: {t1-t0:.2f}s\")\n",
    "print(f\"events: {sum(out[k]['num_entries'] for k in out)}\")\n",
    "event_rate = sum(out[k][\"num_entries\"] for k in out)\n",
    "\n",
    "event_rate = event_rate / (t1-t0)\n",
    "print(f\"event rate: {event_rate / 1_000:.2f} kHz\")\n",
    "\n",
    "read_GB = ak.sum([v['performance_counters']['num_requested_bytes'] for v in report.values()]) / 1_000**3\n",
    "rate_Gbs = read_GB / (t1-t0)\n",
    "print(f\" - read {read_GB:.2f} GB in {t1-t0:.2f} s -> {rate_Gbs:.2f} GBps\")\n",
    "\n",
    "\n",
    "with open(\"outs.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump((out, report), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362ded3-1f2f-4d92-b524-f89bc094a5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stand-alone dask graph example\n",
    "# funciton calls are tuples which first element is Callable type\n",
    "# keys are any hashable that is not a function call\n",
    "# values computed are keys, function calls, lists, tuples. Other values are taken as they are.\n",
    "# arguments in tuple function calls are interpreted as key if needed.\n",
    "\n",
    "graph = {\n",
    "    \"bases\":           list(range(0, 10)),\n",
    "    \"exponents\":       list(range(0, 10)),\n",
    "    \"even_exponents\":  (lambda exps: [e for e in exps if e % 2 == 0], \"exponents\"),\n",
    "    \"even_powers\":     (lambda bs, exps: { b: [ b ** e for e in exps] for b in bs }, \"bases\", \"exponents\"),\n",
    "    \"keyp\":            (lambda e, o: {\"even\": e, \"odd\": o}, \"even_exponents\", \"odd_exponents\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12316a4e-6b62-459a-a81c-7808cb4a47c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vine_get(graph, [\"even_powers\", \"keyp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfae3d9-2c4d-48b1-8d89-c9730670e5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "python-taskvine",
   "language": "python",
   "name": "python-taskvine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
