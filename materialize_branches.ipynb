{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e578b0c-a7cf-429d-94e3-b0641970b2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward: 2.6.3\n",
      "dask-awkward: 2024.3.0\n",
      "uproot: 5.3.11.dev14+g6ff2fe2\n",
      "hist: 2.7.2\n",
      "coffea: 2024.4.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "from dask.distributed import Client, performance_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import utils  # worker count tracking\n",
    "\n",
    "executor = \"dask\"   # \"dask\" or \"taskvine\" or \"dask_gateway\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "\n",
    "    \n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f891e515-6bef-4ac3-a72f-9fec2b13c25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler_options = {}\n",
    "\n",
    "# for coffea-casa\n",
    "if executor == \"taskvine\":\n",
    "    from ndcctools.taskvine import DaskVine, Task\n",
    "    \n",
    "    \n",
    "    manager = DaskVine(port=8786, ssl=True, name=f\"{os.environ.get('USER', 'noname')}-coffea-casa\")\n",
    "\n",
    "    extra_files = {}\n",
    "    env_vars = {}\n",
    "    \n",
    "    token_acc_path = \"/etc/cmsaf-secrets-chown/access_token\"\n",
    "    token_xch_path = \"/etc/cmsaf-secrets-chown/xcache_token\"\n",
    "\n",
    "    if Path(token_acc_path).is_file():\n",
    "        extra_files[manager.declare_file(token_acc_path, cache=True)] = \"access_token\"\n",
    "        env_vars[\"BEARER_TOKEN_FILE\"] = \"access_token\"\n",
    "    if Path(token_xch_path).is_file():\n",
    "        extra_files[manager.declare_file(token_xch_path, cache=True)] = \"xcache_token\"\n",
    "        env_vars[\"XCACHE_FILE\"] = \"xcache_token\"\n",
    "\n",
    "    vine_scheduler = partial(manager.get,\n",
    "                             resources={\"cores\": 1, \"disk\": 2000},  #  max 1 core, 5GB of disk per task\n",
    "                             extra_files=extra_files,\n",
    "                             env_vars=env_vars,\n",
    "                             submit_per_cycle=1000,\n",
    "                             lazy_transfers=True,\n",
    "                             #  resources_mode=None,   # set to \"fixed\" to kill tasks on resources\n",
    "                            )\n",
    "    # change default scheduler\n",
    "    scheduler_options['scheduler'] = vine_scheduler\n",
    "elif executor == \"dask_gateway\":\n",
    "    num_workers = 400   #number of workers desired\n",
    "    from dask.distributed import LocalCluster, Client, progress\n",
    "    from dask_gateway import Gateway\n",
    "    import pathlib\n",
    "    \n",
    "    gateway = Gateway()\n",
    "    clusters=gateway.list_clusters()\n",
    "    cluster = gateway.connect(clusters[0].name)\n",
    "    client = cluster.get_client()\n",
    "    cluster.scale(num_workers)\n",
    "    # %%\n",
    "    def set_env(dask_worker):\n",
    "        path = str(pathlib.Path(dask_worker.local_directory) / 'access_token')\n",
    "        os.environ[\"BEARER_TOKEN_FILE\"] = path\n",
    "        os.chmod(path, 0o600)\n",
    "        os.chmod(\"/etc/grid-security/certificates\", 0o755)\n",
    "\n",
    "    client.wait_for_workers(num_workers)\n",
    "    client.upload_file(\"/etc/cmsaf-secrets/access_token\")\n",
    "    client.run(set_env)\n",
    "        \n",
    "else:\n",
    "    # by default use dask   \n",
    "    # local: single thread, single worker\n",
    "    from dask.distributed import LocalCluster, Client, progress\n",
    "    \n",
    "    # cluster = LocalCluster(n_workers=1, processes=False, threads_per_worker=1)\n",
    "    # client = Client(cluster)\n",
    "    client = Client(\"tls://localhost:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684f1240-a754-4dd1-b861-1bfac65ec288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fname = \"rntuple_files.json\"\n",
    "# fname = \"ttree_files.json\"\n",
    "\n",
    "fileset = {}\n",
    "with open(fname,'r') as fp:\n",
    "    for i,(dataset_name,file_list) in enumerate(json.load(fp).items()):\n",
    "        fileset[dataset_name] = {\"files\": {}}\n",
    "        for j,dataset_fpath in enumerate(file_list):\n",
    "            xrd_fpath = f\"root://eospublic.cern.ch//eos/root-eos/{dataset_fpath}\"\n",
    "            # xrd_fpath = f\"root://xcache.cmsaf-dev.flatiron.hollandhpc.org:1094/{dataset_fpath}\"\n",
    "            fileset[dataset_name][\"files\"][xrd_fpath] = \"Events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e87e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input files after filter: 1\n"
     ]
    }
   ],
   "source": [
    "# apply optional filtering to limit number of input files\n",
    "\n",
    "# limite to the first N files per container, None if no limit\n",
    "LIMIT_NUM_FILES = 10\n",
    "\n",
    "# limit to the first N containers, None if no limit\n",
    "LIMIT_NUM_CONTAINERS = 1\n",
    "\n",
    "fileset = coffea.dataset_tools.max_files(fileset, LIMIT_NUM_FILES)\n",
    "\n",
    "if LIMIT_NUM_CONTAINERS is not None:\n",
    "    fileset = dict((k,v) for i, (k,v) in enumerate(fileset.items()) if i <LIMIT_NUM_CONTAINERS)\n",
    "\n",
    "print(f\"number of input files after filter: {sum([len(f['files']) for f in fileset.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633689aa-419d-4c79-a0b8-b36dad9abd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root://eospublic.cern.ch//eos/root-eos/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root': 'Events'}\n"
     ]
    }
   ],
   "source": [
    "# turn fileset into simple list of files to run over\n",
    "all_files = []\n",
    "for process in fileset:\n",
    "    all_files += fileset[process][\"files\"]\n",
    "    print(fileset[process][\"files\"])\n",
    "\n",
    "# define work to be done\n",
    "def uproot_open_materialize(fname):\n",
    "    BRANCH_LIST = [\n",
    "        \"GenPart_pt\", \n",
    "        # \"GenPart_eta\", \"GenPart_phi\", \"CorrT1METJet_phi\",\n",
    "        # \"GenJet_pt\", \"CorrT1METJet_eta\", \"SoftActivityJet_pt\",\n",
    "        # \"Jet_eta\", \"Jet_phi\", \"SoftActivityJet_eta\", \"SoftActivityJet_phi\", \n",
    "        # \"CorrT1METJet_rawPt\", \"Jet_btagDeepFlavB\", \"GenJet_eta\", \n",
    "        # \"GenPart_mass\", \"GenJet_phi\",\n",
    "        # \"Jet_puIdDisc\", \"CorrT1METJet_muonSubtrFactor\", \"Jet_btagDeepFlavCvL\",\n",
    "        # \"Jet_btagDeepFlavQG\", \"Jet_mass\", \"Jet_pt\", \"GenPart_pdgId\",\n",
    "        # \"Jet_btagDeepFlavCvB\", \"Jet_cRegCorr\"\n",
    "        ]\n",
    "\n",
    "    filter_name = lambda x: x in BRANCH_LIST\n",
    "\n",
    "    size_uncompressed = 0\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        with uproot.open(fname, filter_name=filter_name) as f:\n",
    "            events = f[\"Events\"]\n",
    "            num_entries = events.num_entries\n",
    "            # for b in BRANCH_LIST:\n",
    "                # pass\n",
    "                # events[b].array()\n",
    "                # size_uncompressed += events.arrays([b])[b].uncompressed_bytes\n",
    "                # size_uncompressed += events[b].uncompressed_bytes # TODO: property is not available in RNTuple model.\n",
    "\n",
    "            size_read = f.file.source.num_requested_bytes\n",
    "        exception = None\n",
    "\n",
    "    except:\n",
    "        num_entries = 0\n",
    "        size_read = 0\n",
    "        size_uncompressed = 0\n",
    "        exception = traceback.format_exc()\n",
    "        raise\n",
    "        \n",
    "    t1 = time.perf_counter()\n",
    "    time_finished = datetime.datetime.now()\n",
    "    return {\"fname\": fname, \"read\": size_read, \"uncompressed\": size_uncompressed, \"num_entries\": num_entries,\n",
    "            \"runtime\": t1-t0, \"time_finished\": time_finished, \"exception\": exception}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed69b83-3cab-4bab-86fc-21ae75bf57d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fname': 'root://eospublic.cern.ch//eos/root-eos/AGC/rntuple/nanoAOD/TT_TuneCUETP8M1_13TeV-amcatnlo-pythia8/cmsopendata2015_ttbar_19978_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext1-v1_60000_0004.root',\n",
       "  'read': 127282,\n",
       "  'uncompressed': 0,\n",
       "  'num_entries': 188600,\n",
       "  'runtime': 6.60879027005285,\n",
       "  'time_finished': datetime.datetime(2024, 8, 13, 14, 2, 50, 339624),\n",
       "  'exception': None}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[uproot_open_materialize(f) for f in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35833a73-9c4e-4b74-8029-8b138b0f8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with 1 files\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object has no attribute 'num_entries'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m performance_report(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdask-report-plain-uproot.html\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# futures = client.map(uproot_open_materialize, scattered_data)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# out = ak.Array([r for r in client.gather(iter(futures))])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [dask\u001b[38;5;241m.\u001b[39mdelayed(uproot_open_materialize)(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files]\n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mArray(\u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     13\u001b[0m utils\u001b[38;5;241m.\u001b[39mworker_tracking\u001b[38;5;241m.\u001b[39mstop_tracking_workers()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36muproot_open_materialize\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m uproot\u001b[38;5;241m.\u001b[39mopen(fname, filter_name\u001b[38;5;241m=\u001b[39mfilter_name) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     27\u001b[0m     events \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m     num_entries \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mnum_entries\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# for b in BRANCH_LIST:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# pass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# events[b].array()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# size_uncompressed += events.arrays([b])[b].uncompressed_bytes\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# size_uncompressed += events[b].uncompressed_bytes # TODO: property is not available in RNTuple model.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     size_read \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mnum_requested_bytes\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model_ROOT_3a3a_Experimental_3a3a_RNTuple' object has no attribute 'num_entries'"
     ]
    }
   ],
   "source": [
    "# perform computation\n",
    "print(f\"running with {len(all_files)} files\")\n",
    "# scattered_data = client.scatter([f for f in all_files])  # instead of submitting (possibly big) object directly\n",
    "\n",
    "utils.worker_tracking.start_tracking_workers(client)  # track worker count in background\n",
    "t0 = time.perf_counter()\n",
    "with performance_report(filename=\"dask-report-plain-uproot.html\"):\n",
    "    # futures = client.map(uproot_open_materialize, scattered_data)\n",
    "    # out = ak.Array([r for r in client.gather(iter(futures))])\n",
    "    tasks = [dask.delayed(uproot_open_materialize)(f) for f in all_files]\n",
    "    out = ak.Array(dask.compute(*tasks))\n",
    "t1 = time.perf_counter()\n",
    "utils.worker_tracking.stop_tracking_workers()\n",
    "\n",
    "print(f\"wall clock time: {t1-t0:.2f}s\")\n",
    "print(f\"current number of workers: {len(client.scheduler_info()['workers'])}\")\n",
    "\n",
    "timestamps, nworkers, avg_num_workers = utils.worker_tracking.get_timestamps_and_counts()  # worker count info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b1ad5-5d6c-4845-8ab8-80bb3824156d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.style.use(\"ggplot\")\n",
    "\n",
    "# get arrays for starting time, runtime and end time of all tasks\n",
    "runtimes = np.asarray([datetime.timedelta(seconds=t) for t in out[\"runtime\"]], dtype=np.timedelta64)\n",
    "ends = out[\"time_finished\"].to_numpy()\n",
    "starts = ends - runtimes\n",
    "\n",
    "# calculate instantaneous rates for given timestamp\n",
    "times_for_rates = []\n",
    "instantaneous_rates = []\n",
    "for t in timestamps[::10]:  # only calculate every 10 seconds\n",
    "    mask = np.logical_and((starts <= t), (t <= ends))  # mask for tasks running at given timestamp\n",
    "    rate_Gbps_at_timestamp = sum(out[mask]['read']*8 / 1000**3 / out[mask][\"runtime\"])\n",
    "    # print(f\"instantaneous rate at t={t}: {rate_Gbps_at_timestamp:.2f} Gbps\")\n",
    "    times_for_rates.append(t)\n",
    "    instantaneous_rates.append(rate_Gbps_at_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b37a13-496e-4325-8500-2c65bb08118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of performance\n",
    "read_GB = sum(out['read']) / 1000**3\n",
    "print(f\"total read (compressed): {read_GB:.2f} GB\")\n",
    "print(f\"total read (uncompressed): {sum(out['uncompressed']) / 1000**3:.2f} GB\")\n",
    "\n",
    "rate_Gbps = read_GB*8/(t1-t0)\n",
    "print(f\"average data rate: {rate_Gbps:.2f} Gbps (need to scale by x{200/rate_Gbps:.0f} to reach 200 Gbps)\")\n",
    "\n",
    "n_evts = sum(out[\"num_entries\"])\n",
    "print(f\"total event rate (wall clock time): {n_evts / (t1-t0) / 1000:.2f} kHz (processed {n_evts} events total)\")\n",
    "\n",
    "total_runtime = sum(out[\"runtime\"])\n",
    "print(f\"total aggregated runtime in function: {total_runtime:.2f} s\")\n",
    "print(f\"ratio total runtime / wall clock time: {total_runtime / (t1-t0):.2f} \"\\\n",
    "      \"(should match # cores without overhead / scheduling issues)\")\n",
    "print(f\"time-averaged number of workers: {avg_num_workers:.1f}\")\n",
    "print(f\"\\\"efficiency\\\" (ratio of two numbers above): {total_runtime / (t1-t0) / avg_num_workers:.1%}\")\n",
    "print(f\"event rate (aggregated time spent in function): {n_evts / total_runtime / 1000:.2f} kHz\")\n",
    "\n",
    "utils.worker_tracking.plot_worker_count(timestamps, nworkers, avg_num_workers, times_for_rates, instantaneous_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30523a52-060b-49eb-99cd-66a72b5cc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sum(o is not None for o in out['exception'])} files failed\\n\")\n",
    "\n",
    "# use below to get full list with details\n",
    "# for report in out:\n",
    "#     if report[\"exception\"] is not None:\n",
    "#         print(f\"{report['fname']} failed in {report['runtime']:.2f} s\\n{report['exception']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e28acd-aeb3-45dc-a0cf-30dacddc9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(0, max(out[\"runtime\"])*1.01, 100)\n",
    "ax.hist(out[\"runtime\"], bins=bins)\n",
    "ax.set_xlabel(\"runtime [s]\")\n",
    "ax.set_xlim([0, ax.get_xlim()[1]])\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.semilogy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7a4b5-3513-4d3a-aa6d-37506a48b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[\"num_entries\"], out[\"runtime\"], marker=\"x\")\n",
    "ax.set_xlabel(\"number of events\")\n",
    "ax.set_ylabel(\"runtime [s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0fe35-d324-4ec6-a710-f8a93901ac93",
   "metadata": {},
   "source": [
    "# below: coffea 2024 approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def task(events):\n",
    "    # track number of events\n",
    "    num_events = ak.num(events, axis=0)\n",
    "\n",
    "    # read out all other branches into integers to avoid memory issues\n",
    "    _counter = 0\n",
    "    for obj_to_add in [\n",
    "        events.GenPart.pt,\n",
    "        events.GenPart.eta,\n",
    "        events.GenPart.phi,\n",
    "        events.CorrT1METJet.phi,\n",
    "        events.GenJet.pt, \n",
    "        events.CorrT1METJet.eta,\n",
    "        events.SoftActivityJet.pt,\n",
    "        events.Jet.eta,\n",
    "        events.Jet.phi,\n",
    "        events.SoftActivityJet.eta,\n",
    "        events.SoftActivityJet.phi, \n",
    "        events.CorrT1METJet.rawPt,\n",
    "        events.Jet.btagDeepFlavB,\n",
    "        events.GenJet.eta, \n",
    "        events.GenPart.mass,\n",
    "        events.GenJet.phi,\n",
    "        events.Jet.puIdDisc,\n",
    "        events.CorrT1METJet.muonSubtrFactor,\n",
    "        events.Jet.btagDeepFlavCvL,\n",
    "        events.Jet.btagDeepFlavQG,\n",
    "        events.Jet.mass,\n",
    "        events.Jet.pt,\n",
    "        events.GenPart.pdgId,\n",
    "        events.Jet.btagDeepFlavCvB,\n",
    "        events.Jet.cRegCorr\n",
    "        \n",
    "    ]:\n",
    "        _counter_to_add = ak.count_nonzero(obj_to_add, axis=1)\n",
    "\n",
    "        # reduce >2-dimensional (per event) branches further\n",
    "        for _ in range(_counter_to_add.ndim - 1):\n",
    "            _counter_to_add = ak.count_nonzero(_counter_to_add, axis=-1)\n",
    "\n",
    "        _counter = _counter + _counter_to_add  # sum 1-dim array built from new branch\n",
    "\n",
    "    _counter = ak.count_nonzero(_counter, axis=0)  # reduce to int\n",
    "\n",
    "    return {\"nevts\": num_events, \"_counter\": _counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "samples, report = dataset_tools.preprocess(fileset, skip_bad_files=True, uproot_options={\"allow_read_errors_with_report\": True}, **scheduler_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2544b5-603a-4aa7-b5db-03e6fcd4439c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find issues where access did not work\n",
    "for process in report:\n",
    "    for k, v in report[process][\"files\"].items():\n",
    "        if v[\"steps\"] is None:\n",
    "            print(f\"could not read {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(task, samples, uproot_options={\"allow_read_errors_with_report\": (OSError, TypeError, KeyError)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953d6d",
   "metadata": {},
   "source": [
    "execute task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "utils.worker_tracking.start_tracking_workers(client)  # track worker count in background\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "if executor == \"taskvine\":\n",
    "    ((out, report),) = dask.compute(tasks, **scheduler_options)\n",
    "else:\n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        ((out, report),) = dask.compute(tasks, **scheduler_options)  # feels strange that this is a tuple-of-tuple\n",
    "t1 = time.perf_counter()\n",
    "utils.worker_tracking.stop_tracking_workers()\n",
    "\n",
    "time_uproot = ak.sum([v['duration'] for v in report.values()])\n",
    "print(f\"total time spent in uproot reading data: {time_uproot:.2f} s\")\n",
    "print(f\"wall time: {t1-t0:.2f}s\")\n",
    "\n",
    "timestamps, nworkers, avg_num_workers = utils.worker_tracking.get_timestamps_and_counts()  # worker count info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8c6a3-c938-4629-8ce1-6f50acce1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"output: {out}\")\n",
    "\n",
    "print(\"\\nperformance metrics:\")\n",
    "event_rate = sum([out[process][\"nevts\"] for process in out.keys()]) / (t1-t0)\n",
    "print(f\" - event rate: {event_rate / 1_000:.2f} kHz\")\n",
    "\n",
    "# need uproot>=5.3.2 to get these useful performance stats\n",
    "num_bytes = ak.sum([report[process][\"performance_counters\"][\"num_requested_bytes\"] for process in out.keys()])\n",
    "read_MB = num_bytes / 1_000**2\n",
    "rate_Mbs = read_MB / (t1-t0)\n",
    "print(f\" - read {read_MB:.2f} MB in {t1-t0:.2f} s -> {rate_Mbs*8:.2f} Mbps (need to scale by x{200/8/rate_Mbs*1000:.0f} to reach 200 Gbps)\")\n",
    "print(f\" - time-averaged number of workers: {avg_num_workers:.1f}\")\n",
    "print(f\" - spent {time_uproot:.1f} s reading data with wall time {t1-t0:.2f} and {avg_num_workers:.1f} cores on average -> \\\"efficiency\\\": {time_uproot / (t1-t0) / avg_num_workers:.1%}\")\n",
    "\n",
    "utils.worker_tracking.plot_worker_count(timestamps, nworkers, avg_num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749a960-df4c-4ada-856f-ef228783ceb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# report problematic files that caused exceptions\n",
    "for process in report.keys():\n",
    "    for i_file in range(len(report[process].exception)):\n",
    "        file_report = report[process][i_file]\n",
    "        if file_report.exception is not None:\n",
    "            print(file_report.args[0].strip(\"\\'\"))\n",
    "            print(file_report.message + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
